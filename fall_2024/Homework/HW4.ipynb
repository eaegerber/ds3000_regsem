{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# DS 3000 HW 4"}, {"cell_type": "markdown", "metadata": {}, "source": "Your Name: "}, {"cell_type": "markdown", "metadata": {}, "source": "Due: Tuesday Nov 19 @ 11:59 PM EST\n\nExtra Credit Deadline: Sunday Nov 17 @ 11:59 PM EST\n\n### Submission Instructions\nSubmit this `ipynb` file to Gradescope (this can also be done via the assignment on Canvas).  To ensure that your submitted `ipynb` file represents your latest code, make sure to give a fresh `Kernel > Restart & Run All` just before uploading the `ipynb` file to gradescope. **In addition:**\n- Make sure your name is entered above\n- Make sure you comment your code effectively\n- If problems are difficult for the TAs/Profs to grade, you will lose points\n\n### Tips for success\n- Start early\n- Make use of Piazza (also accessible through Canvas)\n- Make use of Office Hours\n- Remember to use cells and headings to make the notebook easy to read (if a grader cannot find the answer to a problem, you will receive no points for it)\n- Under no circumstances may one student view or share their ungraded homework or quiz with another student [(see also)](http://www.northeastern.edu/osccr/academic-integrity), though you are welcome to **talk about** (*not* show each other your answers to) the problems."}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "# packages that may come in handy\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px"}, {"cell_type": "markdown", "metadata": {}, "source": "# Part 1: Writing Your Own Linear Regression Functions\n\nIn this part, you will write your own linear regression functions that can (and will) be used for Parts 2, 3, and 4. You must make sure that your functions pass the assert statements in each sub-part."}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 1.1: Line of Best Fit Function (10 points)\n\nWrite the function `line_of_best_fit`, including well written docstring, which takes as arguments `X` (an array, either 1-d or 2-d which includes all the predictor values, not including bias term) and `y` (a 1-d array which includes all corresponding response values to `X`) and returns the vector containing the coefficients for the line of best fit, including an intercept term. I have written the `add_bias_column` function below which you will want to use within your `line_of_best_fit` function. **Make sure the assert statement written in the final code cell of this sub-part passes.**"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "def add_bias_column(X):\n    \"\"\"\n    Args:\n        X (array): can be either 1-d or 2-d\n    \n    Returns:\n        Xnew (array): the same array, but 2-d with a column of 1's in the first spot\n    \"\"\"\n    \n    # If the array is 1-d\n    if len(X.shape) == 1:\n        Xnew = np.column_stack([np.ones(X.shape[0]), X])\n    \n    # If the array is 2-d\n    elif len(X.shape) == 2:\n        bias_col = np.ones((X.shape[0], 1))\n        Xnew = np.hstack([bias_col, X])\n        \n    else:\n        raise ValueError(\"Input array must be either 1-d or 2-d\")\n\n    return Xnew"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "X = np.array([0,2,1,0,4])\ny = np.array([3,8,2,3,6])\n\nassert (np.isclose(line_of_best_fit(X, y), np.array([3., 1.]))).all()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 1.2: Prediction and Assessment Function (10 points)\n\nWrite the function `linreg_predict`, including well written docstring, which takes as arguments:\n\n- `Xnew` (an array, either 1-d or 2-d which includes all the $p$ predictor features, not including bias term)\n- `ynew` (a 1-d array which includes all corresponding response values to `Xnew`)\n- `m` (a 1-d array of length $p+1$ which contains the coefficients from the `line_of_best_fit` function)\n\nThe function should return a dictionary containing four key-value pairs:\n\n- `'ypreds'` (the predicted values from applying `m` to `Xnew`)\n- `'resids'` (the residuals, the differences between `ynew` and `ypreds`)\n- `'mse'` (the mean squared error)\n- `'r2'` (the coefficient of determination ($R^2$) representing the proportion of variability in `ynew` explained by the line of best fit\n  - You **do not** have to calculate this manually; you may use the `r2_score` function from `sklearn` (imported for you below)\n\n**Note** you will want to use the `add_bias_column` again within your function before calculating all the outputs in your dictionary. **Also, make sure the assert statement written in the final code cell of this sub-part passes.**"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "from sklearn.metrics import r2_score"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "def compare_dicts(dict1, dict2):\n    if dict1.keys() != dict2.keys():\n        return False\n\n    for key in dict1:\n        if not np.isclose(dict1[key], dict2[key]).all():\n            return False\n\n    return True\n\nexpected_out = {'ypreds': np.array([3., 5., 4., 3., 7.]),\n                'resids': np.array([0., 3., -2., 0., -1.]),\n                'mse': 2.8,\n                'r2': 0.4444444444444444}\n\nX = np.array([0,2,1,0,4])\ny = np.array([3,8,2,3,6])\n\nm = line_of_best_fit(X, y)\nout = linreg_predict(X, y, m)\n\nassert compare_dicts(expected_out, out)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Part 2: Simple Linear Regression\n\nFor this problem you will use the `df_owl_2018.csv` file in your Homework Module on Canvas. This data set contains statistics from the 2018 Overwatch League (cleaned from [this website](https://overwatchleague.com/en-us/statslab?statslab=heroes)). You do not need to be an expert in Overwatch to complete this problem; I will provide all the context you need below. \n\nWe are interested in a specific hero character you can play, Mercy. Mercy's primary purpose is to heal others. Sometimes when she is healing another character, that character manages to defeat an opponent. This counts as a `Defensive Assists` statistic. In this part, we will see if we can predict how many `Defensive Assists` a player achieves as Mercy given the amount of `Healing Done`, in thousands."}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>start_time</th>\n", "      <th>match_id</th>\n", "      <th>stage</th>\n", "      <th>map_type</th>\n", "      <th>map_name</th>\n", "      <th>player</th>\n", "      <th>team</th>\n", "      <th>hero</th>\n", "      <th>role</th>\n", "      <th>Ability Damage Done</th>\n", "      <th>...</th>\n", "      <th>Ultimates Used</th>\n", "      <th>Unscoped Accuracy</th>\n", "      <th>Unscoped Hits</th>\n", "      <th>Unscoped Shots</th>\n", "      <th>Venom Mine Kills</th>\n", "      <th>Weapon Accuracy</th>\n", "      <th>Weapon Kills</th>\n", "      <th>Whole Hog Efficiency</th>\n", "      <th>Whole Hog Kills</th>\n", "      <th>of Rockets Fired</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>2018-01-11 00:12:00</td>\n", "      <td>10223</td>\n", "      <td>Overwatch League - Stage 1</td>\n", "      <td>PAYLOAD</td>\n", "      <td>Dorado</td>\n", "      <td>Agilities</td>\n", "      <td>Los Angeles Valiant</td>\n", "      <td>Genji</td>\n", "      <td>Damage</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>8</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.273585</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2018-01-11 00:12:00</td>\n", "      <td>10223</td>\n", "      <td>Overwatch League - Stage 1</td>\n", "      <td>PAYLOAD</td>\n", "      <td>Dorado</td>\n", "      <td>Danteh</td>\n", "      <td>San Francisco Shock</td>\n", "      <td>Genji</td>\n", "      <td>Damage</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>1</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.166667</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2018-01-11 00:12:00</td>\n", "      <td>10223</td>\n", "      <td>Overwatch League - Stage 1</td>\n", "      <td>PAYLOAD</td>\n", "      <td>Dorado</td>\n", "      <td>Danteh</td>\n", "      <td>San Francisco Shock</td>\n", "      <td>Junkrat</td>\n", "      <td>Damage</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>3</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.137500</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>2018-01-11 00:12:00</td>\n", "      <td>10223</td>\n", "      <td>Overwatch League - Stage 1</td>\n", "      <td>PAYLOAD</td>\n", "      <td>Dorado</td>\n", "      <td>Danteh</td>\n", "      <td>San Francisco Shock</td>\n", "      <td>Tracer</td>\n", "      <td>Damage</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>3</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.327001</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>2018-01-11 00:12:00</td>\n", "      <td>10223</td>\n", "      <td>Overwatch League - Stage 1</td>\n", "      <td>PAYLOAD</td>\n", "      <td>Dorado</td>\n", "      <td>Envy</td>\n", "      <td>Los Angeles Valiant</td>\n", "      <td>D.Va</td>\n", "      <td>Tank</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>23</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.314785</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>5 rows \u00c3\u2014 261 columns</p>\n", "</div>"], "text/plain": ["            start_time  match_id                       stage map_type  \\\n", "0  2018-01-11 00:12:00     10223  Overwatch League - Stage 1  PAYLOAD   \n", "1  2018-01-11 00:12:00     10223  Overwatch League - Stage 1  PAYLOAD   \n", "2  2018-01-11 00:12:00     10223  Overwatch League - Stage 1  PAYLOAD   \n", "3  2018-01-11 00:12:00     10223  Overwatch League - Stage 1  PAYLOAD   \n", "4  2018-01-11 00:12:00     10223  Overwatch League - Stage 1  PAYLOAD   \n", "\n", "  map_name     player                 team     hero    role  \\\n", "0   Dorado  Agilities  Los Angeles Valiant    Genji  Damage   \n", "1   Dorado     Danteh  San Francisco Shock    Genji  Damage   \n", "2   Dorado     Danteh  San Francisco Shock  Junkrat  Damage   \n", "3   Dorado     Danteh  San Francisco Shock   Tracer  Damage   \n", "4   Dorado       Envy  Los Angeles Valiant     D.Va    Tank   \n", "\n", "   Ability Damage Done  ...  Ultimates Used  Unscoped Accuracy  Unscoped Hits  \\\n", "0                  0.0  ...               8                0.0              0   \n", "1                  0.0  ...               1                0.0              0   \n", "2                  0.0  ...               3                0.0              0   \n", "3                  0.0  ...               3                0.0              0   \n", "4                  0.0  ...              23                0.0              0   \n", "\n", "   Unscoped Shots  Venom Mine Kills  Weapon Accuracy  Weapon Kills  \\\n", "0               0                 0         0.273585             0   \n", "1               0                 0         0.166667             0   \n", "2               0                 0         0.137500             0   \n", "3               0                 0         0.327001             0   \n", "4               0                 0         0.314785             0   \n", "\n", "   Whole Hog Efficiency  Whole Hog Kills  of Rockets Fired  \n", "0                   0.0                0               0.0  \n", "1                   0.0                0               0.0  \n", "2                   0.0                0               0.0  \n", "3                   0.0                0               0.0  \n", "4                   0.0                0               0.0  \n", "\n", "[5 rows x 261 columns]"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "import pandas as pd\n\ndf_owl = pd.read_csv('df_owl_2018.csv')\ndf_owl.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 2.1: Data Cleaning (5 points)\n\nBefore starting, we need to do two things:\n1. subset the data set so that it only includes Mercy observations\n    - **hint:** you should remove all `hero` values besides `Mercy`\n2. divide the `Healing Done` column by `1000` so that the values are in thousands of points\n    - this will assist in the interpretation of the slope\n  \nYou do not have to, but it may help (and I recommend) to further clean the data to keep only the two columns we are interested in, `Healing Done` and `Defensive Assists`; or to simply cast those to arrays called `mercy_X` and `mercy_y` (respectively)."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 2.2: Cross Validate, Predict, MSE, and $R^2$ (10 points)\n\nUse the `train_test_split` function I've imported for you below to create a single-fold (70-30 split) cross validation set (i.e. `Xtrain`, `Xtest`, `ytrain`, `ytest`) using the Mercy data you cleaned in Part 2.1. In other words, for example, the `Xtrain` set should contain a random subset of about 70\\% of the `Healing Done` values.\n\nUsing your functions from Part 1, apply the `line_of_best_fit` function to the training data. Then, use the `linreg_predict` function with the test data (and the output from the first function). Print out the resulting cross validated $MSE$ and $R^2$ values and **then**, in a markdown cell, interpret $R^2$ and discuss if you think it is reasonable based on that value to predict `Defensive Assists` with `Healing Done`."}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "from sklearn.model_selection import train_test_split"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 2.3: Plot and Interpret the Full Line (5 points)\n\nNow, fit the regression model to the full data set, then use the `show_fit()` function below to plot it and recover the final best fit line for the model. **Note** that your `line_of_best_fit` function is the only one you need here, and that you will have to remember which value in the output vector is the slope and which is the intercept.\n\n**In a markdown cell**, interpret both the slope and intercept in the context of the problem, explaining what they mean (as best as you can). **Recall** you put `Healing Done` in thousands of points, which will affect the wording of your interpretation of the slope."}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\ndef get_mse(y_true, y_pred):\n    # calculate the mean squared distance between the predicted and actual y\n    return np.mean((y_true - y_pred) ** 2)\n\ndef show_fit(X, y, slope, intercept):\n    plt.figure()\n    \n    # in case this wasn't done before, transform the input data into numpy arrays and flatten them\n    x = np.array(X).ravel()\n    y = np.array(y).ravel()\n    \n    # plot the actual data\n    plt.scatter(x, y, label='data')\n    \n    # compute linear predictions \n    # x is a numpy array so each element gets multiplied by slope and intercept is added\n    y_pred = slope * x + intercept\n    \n    # plot the linear fit\n    plt.plot(x, y_pred, color='black',\n             ls=':',\n             label='linear fit')\n    \n    plt.legend()\n    \n    plt.xlabel('x')\n    plt.ylabel('y')\n    \n    # print the mean squared error\n    y_pred = slope * x + intercept\n    mse = get_mse(y_true=y, y_pred=y_pred)\n    plt.suptitle(f'y_hat = {slope:.3f} * x + {intercept:.3f}, MSE = {mse:.3f}')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 2.4: Make a Prediction (5 points)\n\nDr. Gerber used to play Overwatch back in the day, and would usually play as Mercy (he doesn't like playing games to hurt people, prefers to heal them).\n\nIn the best game Dr. Gerber ever played (and this is true) he did 16223 points of Healing Done. Unfortunately, the video game doesn't tell Dr. Gerber how many Defensive Assists he had in that specific game. Use the model to predict Dr. Gerber's Defensive Assists for him."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 2.5: Check Assumptions and Make Recommendations (5 points)\n\nUse plots to check to see if the residuals meet the assumptions for performing a linear regression:\n1. independence\n2. constant variance/linearity\n3. normality\n\n**Note** that you will want to use your `linreg_predict` function, using the **full data set** as your `Xnew` and `ynew` values, to get the residuals for the purposes of this question.\n\nThen, **in a markdown cell**, write 3-4 sentences about whether the model meets the assumptions, what that tells you about the usefulness of the model, and recommendations for next steps (if any)."}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "import scipy.stats as stats\nimport pylab as py"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Part 3: Multiple Regression\n\nIn Part 3 and Part 4, you will use the `FuelConsumptionCo2.csv` file (from your Homework Module on Canvas) to build two candidate models to predict a vehicle's Carbon Dioxide Emissions (`CO2EMISSIONS`)."}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>MODELYEAR</th>\n", "      <th>MAKE</th>\n", "      <th>MODEL</th>\n", "      <th>VEHICLECLASS</th>\n", "      <th>ENGINESIZE</th>\n", "      <th>CYLINDERS</th>\n", "      <th>TRANSMISSION</th>\n", "      <th>FUELTYPE</th>\n", "      <th>FUELCONSUMPTION_CITY</th>\n", "      <th>FUELCONSUMPTION_HWY</th>\n", "      <th>FUELCONSUMPTION_COMB</th>\n", "      <th>FUELCONSUMPTION_COMB_MPG</th>\n", "      <th>CO2EMISSIONS</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>2014</td>\n", "      <td>ACURA</td>\n", "      <td>ILX</td>\n", "      <td>COMPACT</td>\n", "      <td>2.0</td>\n", "      <td>4</td>\n", "      <td>AS5</td>\n", "      <td>Z</td>\n", "      <td>9.9</td>\n", "      <td>6.7</td>\n", "      <td>8.5</td>\n", "      <td>33</td>\n", "      <td>196</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2014</td>\n", "      <td>ACURA</td>\n", "      <td>ILX</td>\n", "      <td>COMPACT</td>\n", "      <td>2.4</td>\n", "      <td>4</td>\n", "      <td>M6</td>\n", "      <td>Z</td>\n", "      <td>11.2</td>\n", "      <td>7.7</td>\n", "      <td>9.6</td>\n", "      <td>29</td>\n", "      <td>221</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2014</td>\n", "      <td>ACURA</td>\n", "      <td>ILX HYBRID</td>\n", "      <td>COMPACT</td>\n", "      <td>1.5</td>\n", "      <td>4</td>\n", "      <td>AV7</td>\n", "      <td>Z</td>\n", "      <td>6.0</td>\n", "      <td>5.8</td>\n", "      <td>5.9</td>\n", "      <td>48</td>\n", "      <td>136</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>2014</td>\n", "      <td>ACURA</td>\n", "      <td>MDX 4WD</td>\n", "      <td>SUV - SMALL</td>\n", "      <td>3.5</td>\n", "      <td>6</td>\n", "      <td>AS6</td>\n", "      <td>Z</td>\n", "      <td>12.7</td>\n", "      <td>9.1</td>\n", "      <td>11.1</td>\n", "      <td>25</td>\n", "      <td>255</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>2014</td>\n", "      <td>ACURA</td>\n", "      <td>RDX AWD</td>\n", "      <td>SUV - SMALL</td>\n", "      <td>3.5</td>\n", "      <td>6</td>\n", "      <td>AS6</td>\n", "      <td>Z</td>\n", "      <td>12.1</td>\n", "      <td>8.7</td>\n", "      <td>10.6</td>\n", "      <td>27</td>\n", "      <td>244</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   MODELYEAR   MAKE       MODEL VEHICLECLASS  ENGINESIZE  CYLINDERS  \\\n", "0       2014  ACURA         ILX      COMPACT         2.0          4   \n", "1       2014  ACURA         ILX      COMPACT         2.4          4   \n", "2       2014  ACURA  ILX HYBRID      COMPACT         1.5          4   \n", "3       2014  ACURA     MDX 4WD  SUV - SMALL         3.5          6   \n", "4       2014  ACURA     RDX AWD  SUV - SMALL         3.5          6   \n", "\n", "  TRANSMISSION FUELTYPE  FUELCONSUMPTION_CITY  FUELCONSUMPTION_HWY  \\\n", "0          AS5        Z                   9.9                  6.7   \n", "1           M6        Z                  11.2                  7.7   \n", "2          AV7        Z                   6.0                  5.8   \n", "3          AS6        Z                  12.7                  9.1   \n", "4          AS6        Z                  12.1                  8.7   \n", "\n", "   FUELCONSUMPTION_COMB  FUELCONSUMPTION_COMB_MPG  CO2EMISSIONS  \n", "0                   8.5                        33           196  \n", "1                   9.6                        29           221  \n", "2                   5.9                        48           136  \n", "3                  11.1                        25           255  \n", "4                  10.6                        27           244  "]}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": "df_fuel = pd.read_csv('FuelConsumptionCo2.csv')\ndf_fuel.dropna(inplace=True)\ndf_fuel.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 3.1: Data Pre-Processing (10 points)\n\nOur first model will be a multiple regression model where we try to predict `CO2EMISSIONS` with `ENGINESIZE`, `CYLINDERS` and `FUELCONSUMPTION_COMB_MPG`. Create your `X` and `y` arrays. Make sure that:\n\n- You scale the $x$ features **using standardization**\n- You do **not** include a bias column in `X`\n\nDefining the feature list:\n\n    x_feat_list = ['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB_MPG']\n\nmay help. Your `X` should pass the assert statement in the cell before Part 3.2. **Note** if you use a different type of normalization besides standardization (i.e. other than subtracting the mean and dividing all features by their corresponding standard deviations) the assert will not pass."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": "# check if \nassert np.isclose(X[0], np.array([-0.89405053, -0.93629377, 0.83777233])).all()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 3.2: Cross Validation (5 points)\n\nUsing single-fold cross validation with a 70-30 split, create `Xtrain`, `Xtest`, `ytrain`, and `ytest`.\n\nFit the model using **your own** `line_of_best_fit` function to `Xtrain` and `ytrain`.\n\nThen pass `Xtest`, `ytest`, and the output from the `line_of_best_fit` to your `linreg_predict` function, saving that as something. \n\nPrint out the cross-validated $MSE$ and $R^2$ values. You do not have to comment on their values yet; but you will in Part 4.4 as part of comparing this model with the one from Part 4."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 3.3: Checking Assumptions (10 points)\n\nNow fit the full model using your `line_of_best_fit` function, and generate the residuals using your `linreg_predict` function. Create 5 residual plots in order to check that the assumptions of independence, constant variance, and normality are met for the model you built:\n\n- A plot of the index vs. the residuals\n- A plot of `ENGINESIZE` vs. the residuals\n- A plot of `CYLINDERS` vs. the residuals\n- A plot of `FUELCONSUMPTION_COMB_MPG` vs. the residuals\n- A histogram (or normal probability quantile-quantile plot) of the residuals\n\nYou do not have to comment on these plots yet; but you will in Part 4.4 as part of comparing this model with the one from Part 4."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Part 4: Polynomial Regression\n\nWe will compare the model you built in Part 3 with a polynomial regression model where we try to predict `CO2EMISSIONS` with only `FUELCONSUMPTION_COMB_MPG`."}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 4.1: Creating the Design Matrix (5 points)\n\nUse the `PolynomialFeatures` and `.fit_transform` functions to convert the `FUELCONSUMPTION_COMB_MPG` ($x$) feature into an array (**CALL THIS `X_poly`**) that includes **four** columns corresponding to building a quartic model for `CO2EMISSIONS` ($y$) along the lines of: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4$. I have started the process for you by defining the array containing only our target feature, `X_fuel`.\n\n**Note** that the `.fit_transform` function will produce by default **five** columns, including the bias column. Your functions take arrays that do not have this, so you should remove it.\n\nYour `X_poly` should pass the assert statement in the cell before Part 4.2. \n\n**Note**: Do *not* scale your features (it is unnecessary, since there is really only one, albeit raised to different powers, and will cause an assert error)."}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": "from sklearn.preprocessing import PolynomialFeatures\n\nX_fuel = np.array(df_fuel['FUELCONSUMPTION_COMB_MPG']).reshape(-1,1)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": "assert np.isclose(X_poly[0], np.array([33, 1089, 35937, 1185921])).all()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 4.2: Cross Validation (5 points)\n\nUsing single-fold cross validation with a 70-30 split, create `Xtrain`, `Xtest`, `ytrain`, and `ytest` (from `X_poly` from Part 4.1 and `y` as defined before).\n\nFit the model using **your own** `line_of_best_fit` function to `Xtrain` and `ytrain`.\n\nThen pass `Xtest`, `ytest`, and the output from the `line_of_best_fit` to your `linreg_predict` function, saving that as something. \n\nPrint out the cross-validated $MSE$ and $R^2$ values. You do not have to comment on their values yet; but you will in Part 4.4 as part of comparing this model with the one from Part 3."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 4.3: Checking Assumptions (5 points)\n\nNow fit the full model using your `line_of_best_fit` function, and generate the residuals using your `linreg_predict` function. Create 3 residual plots in order to check that the assumptions of independence, constant variance, and normality are met for the model you built:\n\n- A plot of the index vs. the residuals\n- A plot of `FUELCONSUMPTION_COMB_MPG` vs. the residuals\n- A histogram (or normal probability quantile-quantile plot) of the residuals\n\nYou do not have to comment on these plots yet; but you will in Part 4.4 as part of comparing this model with the one from Part 3."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 4.4: Conclusions (10 points)\n\n**In a markdown cell**, give a *lengthy and **detailed*** discussion of the two candidate models from Parts 3 and 4. Discuss each of their strengths/weaknesses/benefits (i.e. which model had the better $R^2$? which had the better $MSE$? which assumptions were met for each model and which were not?). Then, **make a decision** about which model you would suggest (if you **had** to choose) is most appropriate to use for predicting a vehicle's Carbon Dioxide Emissions. Do you have any thoughts about improving either/both of these models? **Discuss this as well.**"}, {"cell_type": "markdown", "metadata": {}, "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}, "nbformat": 4, "nbformat_minor": 4}