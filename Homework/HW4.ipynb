{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# DS 3000 HW 4"}, {"cell_type": "markdown", "metadata": {}, "source": "Due: Tuesday Nov 21 @ 11:59 PM EST\n\n### Submission Instructions\nYou will may submit up to two files for this assignment. This `ipynb` file should have answers to the programming questions, and you could include the answers to the math problems as well either via LaTeX typesetting in Markdown cells, or by embedding images of your written work. If you would rather work on the math problems separately, you may also submit a pdf file with your handwritten answers to the math problems. To ensure that your submitted `ipynb` file represents your latest code, make sure to give a fresh `Kernel > Restart & Run All` just before uploading the `ipynb` file to Gradescope.\n\n### Tips for success\n- Start early\n- Make use of Piazza (also accessible through Canvas)\n- Make use of Office Hours\n- Remember to use cells and headings to make the notebook easy to read (if a grader cannot find the answer to a problem, you will receive no points for it)\n- Under no circumstances may one student view or share their ungraded homework or quiz with another student [(see also)](http://www.northeastern.edu/osccr/academic-integrity), though you are welcome to **talk about** (*not* show each other your answers to) the problems."}, {"cell_type": "markdown", "metadata": {}, "source": "# Part 1: Computation by Hand\n\nFor each of the sub-parts below, you must show all math work/steps (no matter how trivial) to receive full credit. You may either use LaTeX typesetting within a Markdown cell, or do it by hand with pen and paper and embed the image in this .ipynb file, or submit a separate pdf file with your handwritten work. Round all decimals to three places. name:"}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 1.1: Matrix Multiplication (5 points)\n\nUsing the below matrices, perform the following operations by hand, **then** perform the same operations in your notebook using `numpy`. If an operation cannot be done, still write the code but then comment it out before running and submitting your final .ipynb file.\n\n$$A = \\begin{bmatrix}\n-3 & 8 \\\\\n0 & 5\n\\end{bmatrix}$$\n\n$$B = \\begin{bmatrix}\n2 & -7 \\\\\n6 & -1 \\\\\n-9 & 4 \\end{bmatrix}$$\n\n$$C = \\begin{bmatrix}\n-6 & 0 & 5 \\\\\n1 & 3 & -2 \\\\\n7 & -5 & -8 \\\\\n4 & 9 & -10\n\\end{bmatrix}$$\n\n$$D = \\begin{bmatrix}\n-4 & 0 & 3 \\\\\n8 & -2 & 5 \\\\\n6 & -3 & 1\n\\end{bmatrix}$$\n\n$$e = \\begin{bmatrix}\n7 \\\\\n-8 \\\\\n10 \\\\\n-1\n\\end{bmatrix}$$\n\n- $AB^T$\n- $CD$\n- $DB$\n- $Ce$\n- $e^TC$"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 1.2: Spans, Linear In/Dependence, Orthogonality (5 points)\n\nBased on the following three vectors, answer the ensuing questions, making sure to write out all supporting work with by hand or in a markdown cell. You may use `numpy` to help you check some of your answers, if you wish, but all work must be done by hand and provided for full credit.\n\n$$a = \\begin{bmatrix} 8 \\\\ -2 \\end{bmatrix}$$\n$$b = \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}$$\n$$c = \\begin{bmatrix} -3 \\\\ .75 \\end{bmatrix}$$\n\n- What is the span of $a$ and $b$?\n- What is the span of $a$ and $c$?\n- Are the vectors $a$ and $b$ linearly independent or dependent?\n- Is the set of all three vectors linearly independent or dependent?\n- Which vectors are orthogonal to each other?"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 1.3: Projections (5 points)\n\nBy hand, find the point in the span of $a = \\begin{bmatrix} -1 \\\\ 3 \\end{bmatrix}$ that is closest to $b = \\begin{bmatrix} 0 \\\\ -3 \\end{bmatrix}$. Make sure to show **all** work by hand, even if you use `numpy` to verify your answer. **Also, draw a rough sketch** of the operation, including it either as an embedded image in this notebook or in your separate .pdf file."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 1.4: Line of Best Fit (5 points)\n\nYou are interested in if there is a relationship within your friend group between how many siblings they have and how many dates they've been on. You collect the following data from five of your friends:\n\n| siblings | dates |\r\n|----------|-------|\r\n| 0        | 3     |\r\n| 2       9| 8     |\r\n| 1      3 | 2     |\r\n| 0     2  | 3     |\r\n| 4        | 6\n\nFind the line of best fit, by hand, for the relationship treating number of siblings as the $x$ feature and number of dates as the $y$ feature. Be sure to include an intercept term. You may verify your answer using `numpy`, but must show all work by hand.     |"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Part 2: Writing Your Own Linear Regression Functions\n\nIn this part, you will write your own linear regression functions that can (and will) be used for both Part 3 and Part 4. You must make sure that your functions pass the assert statements in each sub-part."}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 2.1: Line of Best Fit Function (10 points)\n\nWrite the function `line_of_best_fit`, including well written docstring, which takes as arguments `X` (an array, either 1-d or 2-d which includes all the predictor values, not including bias term) and `y` (a 1-d array which includes all corresponding response values to `X`) and returns the vector containing the coefficients for the line of best fit, including an intercept term. I have written the `add_bias_column` function below which you will want to use within your `line_of_best_fit` function. **Make sure the assert statement written in the final code cell of this sub-part passes.**"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "def add_bias_column(X):\n    \"\"\"\n    Args:\n        X (array): can be either 1-d or 2-d\n    \n    Returns:\n        Xnew (array): the same array, but 2-d with a column of 1's in the first spot\n    \"\"\"\n    \n    # If the array is 1-d\n    if len(X.shape) == 1:\n        Xnew = np.column_stack([np.ones(X.shape[0]), X])\n    \n    # If the array is 2-d\n    elif len(X.shape) == 2:\n        bias_col = np.ones((X.shape[0], 1))\n        Xnew = np.hstack([bias_col, X])\n        \n    else:\n        raise ValueError(\"Input array must be either 1-d or 2-d\")\n\n    return Xnew"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "X = np.array([0,2,1,0,4])\ny = np.array([3,8,2,3,6])\n\nassert (np.isclose(line_of_best_fit(X, y), np.array([3., 1.]))).all()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 2.2: Prediction and Assessment Function (10 points)\n\nWrite the function `linreg_predict`, including well written docstring, which takes as arguments:\n\n- `Xnew` (an array, either 1-d or 2-d which includes all the $p$ predictor features, not including bias term)\n- `ynew` (a 1-d array which includes all corresponding response values to `Xnew`)\n- `m` (a 1-d array of length $p+1$ which contains the coefficients from the `line_of_best_fit` function)\n\nThe function should return a dictionary containing four key-value pairs:\n\n- `'ypreds'` (the predicted values from applying `m` to `Xnew`)\n- `'resids'` (the residuals, the differences between `ynew` and `ypreds`)\n- `'mse'` (the mean squared error)\n- `'r2'` (the coefficient of determination ($R^2$) representing the proportion of variability in `ynew` explained by the line of best fit\n  - You **do not** have to calculate this manually; you may use the `r2_score` function from `sklearn` (imported for you below)\n\n**Note** you will want to use the `add_bias_column` again within your function before calculating all the outputs in your dictionary. **Also, make sure the assert statement written in the final code cell of this sub-part passes.**"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "from sklearn.metrics import r2_score"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "def compare_dicts(dict1, dict2):\n    if dict1.keys() != dict2.keys():\n        return False\n\n    for key in dict1:\n        if not np.isclose(dict1[key], dict2[key]).all():\n            return False\n\n    return True\n\nexpected_out = {'ypreds': np.array([3., 5., 4., 3., 7.]),\n                'resids': np.array([0., 3., -2., 0., -1.]),\n                'mse': 2.8,\n                'r2': 0.4444444444444444}\n\nX = np.array([0,2,1,0,4])\ny = np.array([3,8,2,3,6])\n\nm = line_of_best_fit(X, y)\nout = linreg_predict(X, y, m)\n\nassert compare_dicts(expected_out, out)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Part 3: Overwatch Simple Linear Regression\n\nFor this problem you will use the `df_owl_2018.csv` file in your Homework Module on Canvas. This data set contains statistics from the 2018 Overwatch League (cleaned from [this website](https://overwatchleague.com/en-us/statslab?statslab=heroes)). You do not need to be an expert in Overwatch to complete this problem; I will provide all the context you need below. \n\nWe are interested in a specific hero character you can play, Mercy. Mercy's primary purpose is to heal others. Sometimes when she is healing another character, that character manages to defeat an opponent. This counts as a `Defensive Assists` statistic. In this part, we will see if we can predict how many `Defensive Assists` a player achieves as Mercy given the amount of `Healing Done`, in thousands."}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>start_time</th>\n", "      <th>match_id</th>\n", "      <th>stage</th>\n", "      <th>map_type</th>\n", "      <th>map_name</th>\n", "      <th>player</th>\n", "      <th>team</th>\n", "      <th>hero</th>\n", "      <th>role</th>\n", "      <th>Ability Damage Done</th>\n", "      <th>...</th>\n", "      <th>Ultimates Used</th>\n", "      <th>Unscoped Accuracy</th>\n", "      <th>Unscoped Hits</th>\n", "      <th>Unscoped Shots</th>\n", "      <th>Venom Mine Kills</th>\n", "      <th>Weapon Accuracy</th>\n", "      <th>Weapon Kills</th>\n", "      <th>Whole Hog Efficiency</th>\n", "      <th>Whole Hog Kills</th>\n", "      <th>of Rockets Fired</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>2018-01-11 00:12:00</td>\n", "      <td>10223</td>\n", "      <td>Overwatch League - Stage 1</td>\n", "      <td>PAYLOAD</td>\n", "      <td>Dorado</td>\n", "      <td>Agilities</td>\n", "      <td>Los Angeles Valiant</td>\n", "      <td>Genji</td>\n", "      <td>Damage</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>8</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.273585</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2018-01-11 00:12:00</td>\n", "      <td>10223</td>\n", "      <td>Overwatch League - Stage 1</td>\n", "      <td>PAYLOAD</td>\n", "      <td>Dorado</td>\n", "      <td>Danteh</td>\n", "      <td>San Francisco Shock</td>\n", "      <td>Genji</td>\n", "      <td>Damage</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>1</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.166667</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2018-01-11 00:12:00</td>\n", "      <td>10223</td>\n", "      <td>Overwatch League - Stage 1</td>\n", "      <td>PAYLOAD</td>\n", "      <td>Dorado</td>\n", "      <td>Danteh</td>\n", "      <td>San Francisco Shock</td>\n", "      <td>Junkrat</td>\n", "      <td>Damage</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>3</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.137500</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>2018-01-11 00:12:00</td>\n", "      <td>10223</td>\n", "      <td>Overwatch League - Stage 1</td>\n", "      <td>PAYLOAD</td>\n", "      <td>Dorado</td>\n", "      <td>Danteh</td>\n", "      <td>San Francisco Shock</td>\n", "      <td>Tracer</td>\n", "      <td>Damage</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>3</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.327001</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>2018-01-11 00:12:00</td>\n", "      <td>10223</td>\n", "      <td>Overwatch League - Stage 1</td>\n", "      <td>PAYLOAD</td>\n", "      <td>Dorado</td>\n", "      <td>Envy</td>\n", "      <td>Los Angeles Valiant</td>\n", "      <td>D.Va</td>\n", "      <td>Tank</td>\n", "      <td>0.0</td>\n", "      <td>...</td>\n", "      <td>23</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0.314785</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "      <td>0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>5 rows \u00c3\u2014 261 columns</p>\n", "</div>"], "text/plain": ["            start_time  match_id                       stage map_type  \\\n", "0  2018-01-11 00:12:00     10223  Overwatch League - Stage 1  PAYLOAD   \n", "1  2018-01-11 00:12:00     10223  Overwatch League - Stage 1  PAYLOAD   \n", "2  2018-01-11 00:12:00     10223  Overwatch League - Stage 1  PAYLOAD   \n", "3  2018-01-11 00:12:00     10223  Overwatch League - Stage 1  PAYLOAD   \n", "4  2018-01-11 00:12:00     10223  Overwatch League - Stage 1  PAYLOAD   \n", "\n", "  map_name     player                 team     hero    role  \\\n", "0   Dorado  Agilities  Los Angeles Valiant    Genji  Damage   \n", "1   Dorado     Danteh  San Francisco Shock    Genji  Damage   \n", "2   Dorado     Danteh  San Francisco Shock  Junkrat  Damage   \n", "3   Dorado     Danteh  San Francisco Shock   Tracer  Damage   \n", "4   Dorado       Envy  Los Angeles Valiant     D.Va    Tank   \n", "\n", "   Ability Damage Done  ...  Ultimates Used  Unscoped Accuracy  Unscoped Hits  \\\n", "0                  0.0  ...               8                0.0              0   \n", "1                  0.0  ...               1                0.0              0   \n", "2                  0.0  ...               3                0.0              0   \n", "3                  0.0  ...               3                0.0              0   \n", "4                  0.0  ...              23                0.0              0   \n", "\n", "   Unscoped Shots  Venom Mine Kills  Weapon Accuracy  Weapon Kills  \\\n", "0               0                 0         0.273585             0   \n", "1               0                 0         0.166667             0   \n", "2               0                 0         0.137500             0   \n", "3               0                 0         0.327001             0   \n", "4               0                 0         0.314785             0   \n", "\n", "   Whole Hog Efficiency  Whole Hog Kills  of Rockets Fired  \n", "0                   0.0                0               0.0  \n", "1                   0.0                0               0.0  \n", "2                   0.0                0               0.0  \n", "3                   0.0                0               0.0  \n", "4                   0.0                0               0.0  \n", "\n", "[5 rows x 261 columns]"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "import pandas as pd\n\ndf_owl = pd.read_csv('df_owl_2018.csv')\ndf_owl.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 3.1: Data Cleaning (5 points)\n\nBefore starting, we need to do two things:\n1. subset the data set so that it only includes Mercy observations\n    - **hint:** you should remove all `hero` values besides `Mercy`\n2. divide the `Healing Done` column by `1000` so that the values are in thousands of points\n    - this will assist in the interpretation of the slope\n  \nYou do not have to, but it may help (and I recommend) to further clean the data to keep only the two columns we are interested in, `Healing Done` and `Defensive Assists`; or to simply cast those to arrays called `mercy_X` and `mercy_y` (respectively)."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 3.2: Cross Validate, Predict, MSE, and $R^2$ (10 points)\n\nUse the `train_test_split` function I've imported for you below to create a single-fold (70-30 split) cross validation set (i.e. `Xtrain`, `Xtest`, `ytrain`, `ytest`) using the Mercy data you cleaned in Part 3.1. In other words, for example, the `Xtrain` set should contain a random subset of about 70\\% of the `Healing Done` values.\n\nUsing your functions from Part 2, apply the `line_of_best_fit` function to the training data. Then, use the `linreg_predict` function with the test data (and the output from the first function). Print out the resulting cross validated $MSE$ and $R^2$ values and **then**, in a markdown cell, interpret $R^2$ and discuss if you think it is reasonable based on that value to predict `Defensive Assists` with `Healing Done`."}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": "from sklearn.model_selection import train_test_split"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 3.3: Use `sklearn` and Compare (5 points)\n\nUse the `LinearRegression` function from `sklearn` to fit the training data (redefined below as `Xtrain_sklearn`, assuming you named it `Xtrain` in Part 3.2) and then use the `.predict` function to predict the $y$-values for the test data (redefined below as `Xtest_sklearn`, assuming you named it `Xtest` in Part 3.2).\n\nUse the provided `get_mse` function to print out the $MSE$ and then use the `r2_score` function to calculate the $R^2$ value. Compare these to the values you got from your own functions in the previous part. Are they the same? Discuss why they are or why they are not **in a markdown cell**."}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "from sklearn.linear_model import LinearRegression\nXtrain_sklearn = Xtrain.reshape(-1, 1)\nXtest_sklearn = Xtest.reshape(-1, 1)\n\ndef get_mse(y_true, y_pred):\n    # calculate the mean squared distance between the predicted and actual y\n    return np.mean((y_true - y_pred) ** 2)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 3.4: Plot and Interpret the Full Line (5 points)\n\nNow, fit the regression model to the full data set, then use the `show_fit()` function below to plot it and recover the final best fit line for the model. **Note** that your `line_of_best_fit` function is the only one you need here, and that you will have to remember which value in the output vector is the slope and which is the intercept.\n\n**Note Also** that you will need to do Part 3.3 (or, at least get use the `get_mse` function) for this to work.\n\n**In a markdown cell**, interpret both the slope and intercept in the context of the problem, explaining what they mean (as best as you can). **Recall** you put `Healing Done` in thousands of points, which will affect the wording of your interpretation of the slope."}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\ndef show_fit(X, y, slope, intercept):\n    plt.figure()\n    \n    # in case this wasn't done before, transform the input data into numpy arrays and flatten them\n    x = np.array(X).ravel()\n    y = np.array(y).ravel()\n    \n    # plot the actual data\n    plt.scatter(x, y, label='data')\n    \n    # compute linear predictions \n    # x is a numpy array so each element gets multiplied by slope and intercept is added\n    y_pred = slope * x + intercept\n    \n    # plot the linear fit\n    plt.plot(x, y_pred, color='black',\n             ls=':',\n             label='linear fit')\n    \n    plt.legend()\n    \n    plt.xlabel('x')\n    plt.ylabel('y')\n    \n    # print the mean squared error\n    y_pred = slope * x + intercept\n    mse = get_mse(y_true=y, y_pred=y_pred)\n    plt.suptitle(f'y_hat = {slope:.3f} * x + {intercept:.3f}, MSE = {mse:.3f}')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 3.5: Check Assumptions and Make Recommendations (5 points)\n\nUse plots to check to see if the residuals meet the assumptions for performing a linear regression:\n1. independence\n2. constant variance/linearity\n3. normality\n\n**Note** that you will want to use your `linreg_predict` function, using the **full data set** as your `Xnew` and `ynew` values, to get the residuals for the purposes of this question.\n\nThen, **in a markdown cell**, write 3-4 sentences about whether the model meets the assumptions, what that tells you about the usefulness of the model, and recommendations for next steps (if any)."}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": "import scipy.stats as stats\nimport pylab as py"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "# Part 4: Fuel Emissions Regression\n\nIn this problem you will use the `FuelConsumptionCo2.csv` file (from your Homework Module on Canvas) to build two candidate models to predict a vehicle's Carbon Dioxide Emissions (`CO2EMISSIONS`)."}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>MODELYEAR</th>\n", "      <th>MAKE</th>\n", "      <th>MODEL</th>\n", "      <th>VEHICLECLASS</th>\n", "      <th>ENGINESIZE</th>\n", "      <th>CYLINDERS</th>\n", "      <th>TRANSMISSION</th>\n", "      <th>FUELTYPE</th>\n", "      <th>FUELCONSUMPTION_CITY</th>\n", "      <th>FUELCONSUMPTION_HWY</th>\n", "      <th>FUELCONSUMPTION_COMB</th>\n", "      <th>FUELCONSUMPTION_COMB_MPG</th>\n", "      <th>CO2EMISSIONS</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>2014</td>\n", "      <td>ACURA</td>\n", "      <td>ILX</td>\n", "      <td>COMPACT</td>\n", "      <td>2.0</td>\n", "      <td>4</td>\n", "      <td>AS5</td>\n", "      <td>Z</td>\n", "      <td>9.9</td>\n", "      <td>6.7</td>\n", "      <td>8.5</td>\n", "      <td>33</td>\n", "      <td>196</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2014</td>\n", "      <td>ACURA</td>\n", "      <td>ILX</td>\n", "      <td>COMPACT</td>\n", "      <td>2.4</td>\n", "      <td>4</td>\n", "      <td>M6</td>\n", "      <td>Z</td>\n", "      <td>11.2</td>\n", "      <td>7.7</td>\n", "      <td>9.6</td>\n", "      <td>29</td>\n", "      <td>221</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2014</td>\n", "      <td>ACURA</td>\n", "      <td>ILX HYBRID</td>\n", "      <td>COMPACT</td>\n", "      <td>1.5</td>\n", "      <td>4</td>\n", "      <td>AV7</td>\n", "      <td>Z</td>\n", "      <td>6.0</td>\n", "      <td>5.8</td>\n", "      <td>5.9</td>\n", "      <td>48</td>\n", "      <td>136</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>2014</td>\n", "      <td>ACURA</td>\n", "      <td>MDX 4WD</td>\n", "      <td>SUV - SMALL</td>\n", "      <td>3.5</td>\n", "      <td>6</td>\n", "      <td>AS6</td>\n", "      <td>Z</td>\n", "      <td>12.7</td>\n", "      <td>9.1</td>\n", "      <td>11.1</td>\n", "      <td>25</td>\n", "      <td>255</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>2014</td>\n", "      <td>ACURA</td>\n", "      <td>RDX AWD</td>\n", "      <td>SUV - SMALL</td>\n", "      <td>3.5</td>\n", "      <td>6</td>\n", "      <td>AS6</td>\n", "      <td>Z</td>\n", "      <td>12.1</td>\n", "      <td>8.7</td>\n", "      <td>10.6</td>\n", "      <td>27</td>\n", "      <td>244</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   MODELYEAR   MAKE       MODEL VEHICLECLASS  ENGINESIZE  CYLINDERS  \\\n", "0       2014  ACURA         ILX      COMPACT         2.0          4   \n", "1       2014  ACURA         ILX      COMPACT         2.4          4   \n", "2       2014  ACURA  ILX HYBRID      COMPACT         1.5          4   \n", "3       2014  ACURA     MDX 4WD  SUV - SMALL         3.5          6   \n", "4       2014  ACURA     RDX AWD  SUV - SMALL         3.5          6   \n", "\n", "  TRANSMISSION FUELTYPE  FUELCONSUMPTION_CITY  FUELCONSUMPTION_HWY  \\\n", "0          AS5        Z                   9.9                  6.7   \n", "1           M6        Z                  11.2                  7.7   \n", "2          AV7        Z                   6.0                  5.8   \n", "3          AS6        Z                  12.7                  9.1   \n", "4          AS6        Z                  12.1                  8.7   \n", "\n", "   FUELCONSUMPTION_COMB  FUELCONSUMPTION_COMB_MPG  CO2EMISSIONS  \n", "0                   8.5                        33           196  \n", "1                   9.6                        29           221  \n", "2                   5.9                        48           136  \n", "3                  11.1                        25           255  \n", "4                  10.6                        27           244  "]}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": "df_fuel = pd.read_csv('FuelConsumptionCo2.csv')\ndf_fuel.dropna(inplace=True)\ndf_fuel.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 4.1: Multiple Regression (10 points)\n\nOur first model will be a multiple regression model where we try to predict `CO2EMISSIONS` with `ENGINESIZE`, `CYLINDERS` and `FUELCONSUMPTION_COMB_MPG`. Be sure to complete all the following steps:"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Part 4.1.1\n\nCreate your `X` and `y` arrays. Make sure that:\n\n- You scale the $x$ features **using scale normalization**\n- You do **not** include a bias column in `X`\n\nDefining the feature list:\n\n    x_feat_list = ['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB_MPG']\n\nmay help. Your `X` should pass the assert statement in the cell before Part 4.1.2. **Note** if you use a different type of normalization besides scale normalization (i.e. other than simply dividing all features by their corresponding standard deviations) the assert will not pass."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": "# check if \nassert np.isclose(X[0], np.array([1.41531251, 2.19568706, 4.77566865])).all()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Part 4.1.2\n\nUsing single-fold cross validation with a 70-30 split, create `Xtrain`, `Xtest`, `ytrain`, and `ytest`.\n\nFit the model using **your own** `line_of_best_fit` function to `Xtrain` and `ytrain`.\n\nThen pass `Xtest`, `ytest`, and the output from the `line_of_best_fit` to your `linreg_predict` function, saving that as something. \n\nPrint out the cross-validated $MSE$ and $R^2$ values. You do not have to comment on their values yet; but you will in Part 4.3 as part of comparing this model with the one from Part 4.2."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "#### Part 4.1.3\n\nNow fit the full model using your `line_of_best_fit` function, and generate the residuals using your `linreg_predict` function. Create 5 residual plots in order to check that the assumptions of independence, constant variance, and normality are met for the model you built:\n\n- A plot of the index vs. the residuals\n- A plot of `ENGINESIZE` vs. the residuals\n- A plot of `CYLINDERS` vs. the residuals\n- A plot of `FUELCONSUMPTION_COMB_MPG` vs. the residuals\n- A normal probability quantile-quantile plot of the residuals\n\nYou do not have to comment on these plots yet; but you will in Part 4.3 as part of comparing this model with the one from Part 4.2."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 4.2: Polynomial Regression (10 points)\n\nOur second model will be a polynomial regression model where we try to predict `CO2EMISSIONS` with `FUELCONSUMPTION_COMB_MPG`. Be sure to complete all the following steps:"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Part 4.2.1\n\nUse the `PolynomialFeatures` and `.fit_transform` functions to convert the `FUELCONSUMPTION_COMB_MPG` ($x$) feature into an array (**CALL THIS `X_poly`**) that includes **four** columns corresponding to building a quartic model for `CO2EMISSIONS` ($y$) along the lines of: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4$. I have started the process for you by defining the array containing only our target feature, `X_fuel`.\n\n**Note** that the `.fit_transform` function will produce by default **five** columns, including the bias column. Your functions take arrays that do not have this, so you should remove it.\n\nYour `X_poly` should pass the assert statement in the cell before Part 4.2.2. **Note**: Do *not* scale your features (it is unnecessary, since there is really only one, albeit raised to different powers, and will cause an assert error)."}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": "from sklearn.preprocessing import PolynomialFeatures\n\nX_fuel = np.array(df_fuel['FUELCONSUMPTION_COMB_MPG']).reshape(-1,1)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": "assert np.isclose(X_poly[0], np.array([33, 1089, 35937, 1185921])).all()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Part 4.2.2\n\nUsing single-fold cross validation with a 70-30 split, create `Xtrain`, `Xtest`, `ytrain`, and `ytest` (from `X_poly` from Part 4.2.1 and `y` as defined before).\n\nFit the model using **your own** `line_of_best_fit` function to `Xtrain` and `ytrain`.\n\nThen pass `Xtest`, `ytest`, and the output from the `line_of_best_fit` to your `linreg_predict` function, saving that as something. \n\nPrint out the cross-validated $MSE$ and $R^2$ values. You do not have to comment on their values yet; but you will in Part 4.3 as part of comparing this model with the one from Part 4.1."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "#### Part 4.2.3\n\nNow fit the full model using your `line_of_best_fit` function, and generate the residuals using your `linreg_predict` function. Create 3 residual plots in order to check that the assumptions of independence, constant variance, and normality are met for the model you built:\n\n- A plot of the index vs. the residuals\n- A plot of `FUELCONSUMPTION_COMB_MPG` vs. the residuals\n- A normal probability quantile-quantile plot of the residuals\n\nYou do not have to comment on these plots yet; but you will in Part 4.3 as part of comparing this model with the one from Part 4.1."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "## Part 4.3: Conclusions (10 points)\n\n**In a markdown cell**, give a *lengthy and **detailed*** discussion of the two candidate models. Discuss each of their strengths/weaknesses/benefits (i.e. which model had the better $R^2$? which had the better $MSE$? which assumptions were met for each model and which were not?). Then, **make a decision** about which model you would suggest (if you **had** to choose) is most appropriate to use for predicting a vehicle's Carbon Dioxide Emissions. Do you have any thoughts about improving either/both of these models? **Discuss this as well.**"}, {"cell_type": "markdown", "metadata": {}, "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.6"}}, "nbformat": 4, "nbformat_minor": 4}