{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52a63c5-8632-42cf-a598-1f9b9fcf118e",
   "metadata": {},
   "source": [
    "# Lab 4 (Due @ by 11:59 pm via Canvas/Gradescope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f3831-9eab-4c04-a0e5-dbe043d32320",
   "metadata": {},
   "source": [
    "Due: Tuesday Nov 28 @ 11:59 PM EST\n",
    "\n",
    "### Submission Instructions\n",
    "Submit this `ipynb` file to Gradescope (this can also be done via the assignment on Canvas).  To ensure that your submitted `ipynb` file represents your latest code, make sure to give a fresh `Kernel > Restart & Run All` just before uploading the `ipynb` file to gradescope.\n",
    "\n",
    "### Group Work\n",
    "\n",
    "You are encouraged to work in groups for this Lab, however each student should submit their own notebook file to Gradescope. While each Part of the Lab depends on previous parts, talking through the problem with your group should help speed up both understanding and arriving at a solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c2500-de83-4732-a3ac-78ccdd6094f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might use the below modules on this lab\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import scipy.stats as stats\n",
    "import pylab as py\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197c58c-8285-446e-bb07-347de63ce276",
   "metadata": {},
   "source": [
    "## Part 1: Two Candidate Models (36 points)\n",
    "\n",
    "For this lab, you will use [this data set from kaggle](https://www.kaggle.com/datasets/hijest/covid19-public-health-social-measures/data) which has been cleaned and is available in the Labs Module on Canvas (called `phsm-severity-data.csv`). It contains data concerning COVID-19 Public Health Social Measures (PHSM) and a Global Severity Index which measures how severe COVID-19 cases were in a given country from the end of 2019 to August 2021. The features of interest are (with the corresponding notation for this problem):\n",
    "\n",
    "- $y$: `GLOBAL_INDEX`, the severity index for the country at a specific date (range 1-88, lower values indicate less severe cases)\n",
    "- $x_1$: `MASKS`, the enforcement of mask wearing as a measure (range 0-100, larger values indicate more enforcement)\n",
    "- $x_2$: `TRAVEL`, the enforcement of travel restrictions as a measure (range 0-100)\n",
    "- $x_3$: `GATHERINGS`, the enforcement of limiting large gatherings as a measure (range 0-50)\n",
    "- $x_4$: `SCHOOLS`, the enforcement of school closures as a measure (range 0-100)\n",
    "- $x_5$: `BUSINESSES`, the enforcement of business closures as a measure (range 0-133)\n",
    "- $x_6$: `MOVEMENTS`, the enforcement of mask wearing as a measure (range 0-100)\n",
    "\n",
    "Assume (for convenience) that all the features are already on the same scale. You have been discussing building a model to predict the `GLOBAL_INDEX` with the other numeric features with a couple experts. Here are what they say:\n",
    "\n",
    "- **Expert 1:** \"Keep in mind that what you'll see is that the features tend to have a positive impact on the index, which is to be expected (more enforcement generally correlates with more severe cases). Based on my own studies, I believe that all the features **except** `GATHERINGS` and `SCHOOLS` should be useful for predicting, and effect the `GLOBAL_INDEX` **linearly**, except that I believe `MASKS` should have a **cubic** effect; countries with no or low mask mandates should actually be more severe than those with some, though masking and severity should then rise together for awhile before leveling off.\"\n",
    "- **Expert 2:** \"I agree mostly with Expert 1, except I believe that `SCHOOLS` **should be** and `TRAVEL` **should not be** included, and I actually expect that `BUSINESSES` should be **quadratically** related to `GLOBAL_INDEX`; while severity and business closures should initially rise together, I expect there to be a leveling off towards the higher levels.\"\n",
    "\n",
    "Based on these two experts' opinions, you will build the two candidate models. To start investigating them, perform the following tasks:\n",
    "\n",
    "1. Read in the data set `phsm-severity-data.csv` into this notebook file using `pandas`, call it `df_covid`.\n",
    "2. In a markdown cell, write out each of the models suggested by the experts in mathematical notation (i.e., a model including `MASKS` (cubicly) and `TRAVEL` would be: $\\hat{y} = b_0 + b_1x_1 + b_2x_1^2 + b_3x_1^3 + b_4x_2$)\n",
    "3. Create three scatter plots to investigate the relationships between (a) `TRAVEL` and `GLOBAL_INDEX`, (b) `SCHOOLS` and `GLOBAL_INDEX`, and (c) between `BUSINESSES` and `GLOBAL_INDEX`.\n",
    "4. In a markdown cell, discuss whether these plots give you any intuition as to which model suggested by the experts may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa175cbd-60ef-473e-afa3-f28d4575f8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af10e32-130c-42f3-a923-bc891516d3a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae1ed8e-439b-4633-947c-ba5fce309d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "251b4050-7d25-4e76-868d-0e336f95707a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1aac814-940d-4a04-9614-48af99c4b92a",
   "metadata": {},
   "source": [
    "# Part 2: Cross Validating to Fit and Compare (24 points)\n",
    "\n",
    "To make this go faster, I have implemented 10-fold cross validation (there are over 100000 observations, so LOO-CV takes too long) to fit each of the candidate models and print out their cross validated $R^2$ values. I have also written the code to produce the relevant residual plots for checking the assumptions. You do not have to write any actual code, however, **you must read through the code** and write **DETAILED** comments on each code chunk where it says `# EXPLAIN THIS CHUNK:`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd53bc-accb-4033-9536-53a1719d7847",
   "metadata": {},
   "source": [
    "## MODEL A (no `GATHERINGS` or `SCHOOLS`, with `TRAVEL`, `BUSINESSES` linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21d57b-f350-4e11-8db2-bdc3906372c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLAIN THIS CHUNK: \n",
    "poly_t = PolynomialFeatures(3)\n",
    "x1_poly = poly_t.fit_transform(df_covid.MASKS.to_numpy().reshape(-1,1))\n",
    "XA = np.column_stack([x1_poly,\n",
    "                      df_covid.TRAVEL,\n",
    "                      df_covid.BUSINESSES,\n",
    "                      df_covid.MOVEMENTS])\n",
    "y = df_covid.GLOBAL_INDEX.to_numpy()\n",
    "\n",
    "y_predsA = np.empty_like(y)\n",
    "\n",
    "# EXPLAIN THIS CHUNK:\n",
    "kfoldA = KFold(n_splits=10)\n",
    "for train_idx, test_idx in kfoldA.split(XA, y):\n",
    "    XA_test = XA[test_idx, :]\n",
    "    XA_train = XA[train_idx, :]\n",
    "    y_train = y[train_idx]\n",
    "     \n",
    "    b_train = np.matmul(np.linalg.inv(np.matmul(XA_train.T, XA_train)), np.matmul(XA_train.T, y_train))\n",
    "     \n",
    "    y_predsA[test_idx] = np.matmul(XA_test, b_train)\n",
    "\n",
    "# EXPLAIN THIS CHUNK:\n",
    "eA = y - y_predsA\n",
    "mseA = np.mean(eA ** 2)\n",
    "r2A = 1 - (mseA/np.var(y))\n",
    "print(r2A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e441a2-6e9e-400a-acec-b8542865dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = ['index', 'MASKS', 'TRAVEL', 'BUSINESSES', 'MOVEMENTS']\n",
    "\n",
    "# EXPLAIN THIS CHUNK: \n",
    "for plot in x_labels:\n",
    "    plot_idx = x_labels.index(plot)\n",
    "    plt.subplot(1, 5, plot_idx+1)\n",
    "    if plot_idx == 0:\n",
    "        plt.scatter(x = range(len(y)), y = eA)\n",
    "        plt.xlabel(plot)\n",
    "        plt.ylabel('residuals')\n",
    "    else:\n",
    "        plt.scatter(x = df_covid.loc[:,plot], y = eA)\n",
    "        plt.xlabel(plot)\n",
    "    \n",
    "plt.gcf().set_size_inches(12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8570d3f4-a981-411d-9643-eec2ff8dfd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(eA, dist=\"norm\", plot=py)\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d880025-da65-4201-96dd-5d466977c342",
   "metadata": {},
   "source": [
    "## MODEL B (no `GATHERINGS` or `TRAVEL`, with `SCHOOLS`, `BUSINESSES` quadratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad82dc-c13a-4e4a-8771-3efcbbdcdbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLAIN THIS CHUNK: \n",
    "poly_t2 = PolynomialFeatures(2)\n",
    "x5_poly = poly_t2.fit_transform(df_covid.BUSINESSES.to_numpy().reshape(-1,1))\n",
    "XB = np.column_stack([x1_poly,\n",
    "                      df_covid.SCHOOLS,\n",
    "                      x5_poly[:,1:],\n",
    "                      df_covid.MOVEMENTS])\n",
    "\n",
    "y_predsB = np.empty_like(y)\n",
    "\n",
    "# EXPLAIN THIS CHUNK:\n",
    "kfoldB = KFold(n_splits=10)\n",
    "for train_idx, test_idx in kfoldB.split(XB, y):\n",
    "    XB_test = XB[test_idx, :]\n",
    "    XB_train = XB[train_idx, :]\n",
    "    y_train = y[train_idx]\n",
    "     \n",
    "    b_train = np.matmul(np.linalg.inv(np.matmul(XB_train.T, XB_train)), np.matmul(XB_train.T, y_train))\n",
    "     \n",
    "    y_predsB[test_idx] = np.matmul(XB_test, b_train)\n",
    "\n",
    "# EXPLAIN THIS CHUNK:\n",
    "eB = y - y_predsB\n",
    "mseB = np.mean(eB ** 2)\n",
    "r2B = 1 - (mseB/np.var(y))\n",
    "print(r2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd370bb-22a9-4620-9c74-bfc470e3aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = ['index', 'MASKS', 'SCHOOLS', 'BUSINESSES', 'MOVEMENTS']\n",
    "\n",
    "# EXPLAIN THIS CHUNK: \n",
    "for plot in x_labels:\n",
    "    plot_idx = x_labels.index(plot)\n",
    "    plt.subplot(1, 5, plot_idx+1)\n",
    "    if plot_idx == 0:\n",
    "        plt.scatter(x = range(len(y)), y = eB)\n",
    "        plt.xlabel(plot)\n",
    "        plt.ylabel('residuals')\n",
    "    else:\n",
    "        plt.scatter(x = df_covid.loc[:,plot], y = eB)\n",
    "        plt.xlabel(plot)\n",
    "    \n",
    "plt.gcf().set_size_inches(12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a1143-9e9c-44db-ad1b-56fe695f9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(eB, dist=\"norm\", plot=py)\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0732994-333f-4192-9690-ddae0b79681f",
   "metadata": {},
   "source": [
    "# Part 3: Summarize and Outline Future Steps (40 points)\n",
    "\n",
    "The code below fits the models (both model A and B) to the full data set and prints out the coefficients. Use these, as well as the results from Part 2 to write a **detailed** paragraph or two of **at least five sentences** which discusses:\n",
    "\n",
    "- Do the slopes that the models have in common (for `MASKS` and `MOVEMENTS`) look similar or different? How would you interpret the coefficients that differentiate (for `TRAVEL`, `SCHOOLS`, and `BUSINESSES`) the models?\n",
    "- How would you interpret the difference in the cross validated $R^2$ values of the two models?\n",
    "- What would you say about how the assumptions were met, or not met, for the two models?\n",
    "- Which expert would you say has the better idea of how the social measures predict severity index and why? (i.e., if you were forced to choose between the two models, which one would you choose and why?)\n",
    "- What sort of improvements do you think you could make to either/both of the models to improve your ability to predict the severity index of COVID-19 in a country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca138d-1740-4780-9729-bee32cc00379",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = np.matmul(np.linalg.inv(np.matmul(XA.T, XA)), np.matmul(XA.T, y))\n",
    "print(modelA.round(2))\n",
    "modelB = np.matmul(np.linalg.inv(np.matmul(XB.T, XB)), np.matmul(XB.T, y))\n",
    "print(modelB.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32075a9a-3f32-4267-94b6-aae9b12d3ab7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
