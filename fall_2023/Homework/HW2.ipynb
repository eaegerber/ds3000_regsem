{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 3000 HW 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due: Tuesday Oct 10 @ 11:59 PM EST\n",
    "\n",
    "### Submission Instructions\n",
    "Submit this `ipynb` file to Gradescope (this can also be done via the assignment on Canvas).  To ensure that your submitted `ipynb` file represents your latest code, make sure to give a fresh `Kernel > Restart & Run All` just before uploading the `ipynb` file to gradescope.\n",
    "\n",
    "### Tips for success\n",
    "- Start early\n",
    "- Make use of Piazza (also accessible through Canvas)\n",
    "- Make use of Office Hours\n",
    "- Remember to use cells and headings to make the notebook easy to read (if a grader cannot find the answer to a problem, you will receive no points for it)\n",
    "- Under no circumstances may one student view or share their ungraded homework or quiz with another student [(see also)](http://www.northeastern.edu/osccr/academic-integrity), though you are welcome to **talk about** (*not* show each other your answers to) the problems.\n",
    "\n",
    "### Finally:\n",
    "\n",
    "I designed this homework to provide **less** guidance in each subsequent part; this is on purpose, so that you slowly get used to thinking more critically about how to approach the various tasks. If you are confused as you are working, especially with the later parts, please don't hesitate to reach out for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Sunrise-Sunset API\n",
    "\n",
    "This first part of the homework asks you to complete the pipeline which, given the lattitude / longitude and timezone of some cities:\n",
    "\n",
    "``` python\n",
    "loc_dict = {'Boston': (42.3601, -71.0589, 'US/Eastern'),\n",
    "            'Lusaka': (-15.3875, 28.3228, 'Africa/Lusaka'),\n",
    "            'Sydney': (-33.8688, 151.2093, 'Australia/Sydney')}\n",
    "```\n",
    "\n",
    "the keys are the `name` of the city and the values are tuples of `lat, lon, timezone_name\n",
    "\n",
    "is able to:\n",
    "- query a sunrise / sunset API\n",
    "- clean and process data (timezone management & building `datetime` objects)\n",
    "- For extra credit: produce the a graph of daylight through the year like this:\n",
    "\n",
    "<img src=\"https://i.ibb.co/CBhWtCY/newdaylight.png\" alt=\"newdaylight\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Getting Sunrise Sunset via API (5 points)\n",
    "Write the `get_sunrise_sunset()` function below so that it uses [this sunrise sunset API](https://sunrise-sunset.org/api) to produce the output (the dictionary) shown in the test case below so that it passes the case.\n",
    "\n",
    "It may be helpful to know that this particular API...\n",
    "- requires no api key\n",
    "- returns about 2.5 queries per second\n",
    "- did not block me when I tried to make 100 consecutive calls as quickly as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have these modules installed\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to write a good docstring! I will do this for you for the other functions in this homework, but you should practice here!\n",
    "def get_sunrise_sunset(lat, lng, date):\n",
    "    \"\"\" WRITE YOUR DOCSTRING HERE\n",
    "    \"\"\"   \n",
    "    \n",
    "    # WRITE YOUR FUNCTIONS HERE AND DELETE THE PASS BELOW\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_dict = get_sunrise_sunset(lat=42.3601, lng=-71.0589, date='2022-02-15')\n",
    "sun_dict_expected = \\\n",
    "{'results': {'sunrise': '11:38:48 AM',\n",
    "            'sunset': '10:17:50 PM',\n",
    "            'solar_noon': '4:58:19 PM',\n",
    "            'day_length': '10:39:02',\n",
    "            'civil_twilight_begin': '11:11:30 AM',\n",
    "            'civil_twilight_end': '10:45:08 PM',\n",
    "            'nautical_twilight_begin': '10:38:37 AM',\n",
    "            'nautical_twilight_end': '11:18:00 PM',\n",
    "            'astronomical_twilight_begin': '10:06:05 AM',\n",
    "            'astronomical_twilight_end': '11:50:33 PM'},\n",
    " 'status': 'OK',\n",
    " 'lat-lng': (42.3601, -71.0589),\n",
    " 'date': '2022-02-15'}\n",
    "\n",
    "assert sun_dict == sun_dict_expected, 'get_sunrise_sunset() error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 Timezone Considerations: (5 points)\n",
    "\n",
    "It may appear the test case above works, but a look at the API's documentation reminds us: \n",
    "\n",
    "    \"NOTE: All times are in UTC and summer time adjustments are not included in the returned data.\"\n",
    "    \n",
    "Meaning that we would need to change the timezone ourself if comparing different locations. \n",
    "\n",
    "Complete the `change_tz()` below so that it passes the given test case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need these\n",
    "import pytz\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have started the function for you\n",
    "def change_tz(dt, timezone_from, timezone_to):\n",
    "    \"\"\" converts timezone of a timezone naive datetime object\n",
    "    \n",
    "    Args:\n",
    "        dt (datetime): datetime (or time) object without timezone\n",
    "        timezone_from (str): timezone of input\n",
    "        timezone_to (str): timezone of output datetime\n",
    "        \n",
    "    Returns:\n",
    "        dt (datetime): datetime object corresponding to \n",
    "            unix_time\n",
    "    \"\"\"\n",
    "    \n",
    "    dt_from = pytz.timezone(timezone_from).localize(dt)\n",
    "    # complete the function and remove the pass below\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test case input / output\n",
    "dt_no_tz = datetime(2021, 2, 13, 9, 54, 4, 270088)\n",
    "dt_expect = datetime(2021, 2, 13, 14, 54, 4, 270088, tzinfo=pytz.timezone('GMT'))\n",
    "\n",
    "# compute actual output\n",
    "dt = change_tz(dt_no_tz, timezone_from='US/Eastern', timezone_to='GMT')\n",
    "\n",
    "assert dt == dt_expect, 'change_tz() error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test case input / output\n",
    "dt_no_tz = datetime(2021, 2, 13, 9, 54, 4, 270088)\n",
    "dt_expect = datetime(2021, 2, 13, 9, 54, 4, 270088, tzinfo=pytz.timezone('GMT'))\n",
    "\n",
    "# compute actual output\n",
    "dt = change_tz(dt_no_tz, timezone_from='GMT', timezone_to='GMT')\n",
    "\n",
    "assert dt == dt_expect, 'change_tz() error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3: Turning the dictionary into a Series (5 points)\n",
    "Build `clean_sun_dict()` to pass each of the two test cases below.  Note that:\n",
    "- sunrise and sunset are `time` objects which account for daylight's saving:\n",
    "    - include the date when building these objects\n",
    "    - use `change_tz()` above to cast them to the proper timezone\n",
    "    - build `time` objects by calling `datetime.time()` to discard the date of a `datetime`\n",
    "    - importing `pandas as pd` and using `pd.to_datetime` may also be helpful\n",
    "- `sunrise_hr` and `sunset_hr` are the hours since the day began in local timezone (more easily graphed)\n",
    "    - you may use `.strftime()` and `int()` to cast time objects to strings and then integers (which may be helpful) \n",
    "    \n",
    "**NOTE:** There may be more than one way to accomplish writing this function; as long as the function passes both `assert` test cases, you may continue. Just do be sure to comment and present your code as cleanly as possible. **NOTE ALSO** that because of the way *I* made the solution, the `sunrise_hr` and `sunset_hr` values are rounded strangely. If you are getting something *close*, you **may** change the test case to match your so that the `assert` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sun_dict(sun_dict, timezone_to):\n",
    "    \"\"\" builds pandas series and cleans output of API\n",
    "    \n",
    "    Args:\n",
    "        sun_dict (dict): dict of json (see ex below)\n",
    "        timezone_to (str): timezone of outputs (API returns\n",
    "            UTC times)\n",
    "            \n",
    "    Returns:\n",
    "        sun_series (pd.Series): all times converted to\n",
    "            time objects\n",
    "    \n",
    "    example sun_series:\n",
    "    \n",
    "    date            2021-02-13 00:00:00\n",
    "    lat-lng        (36.72016, -4.42034)\n",
    "    sunrise                    02:11:06\n",
    "    sunrise_hr                    2.185\n",
    "    sunset                     13:00:34\n",
    "    sunset_hr                   13.0094\n",
    "    dtype: object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Function content here, remove pass below\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_dict = {'results': {'sunrise': '11:38:48 AM',\n",
    "                        'sunset': '10:17:50 PM',\n",
    "                        'solar_noon': '4:58:19 PM',\n",
    "                        'day_length': '10:39:02',\n",
    "                        'civil_twilight_begin': '11:11:30 AM',\n",
    "                        'civil_twilight_end': '10:45:08 PM',\n",
    "                        'nautical_twilight_begin': '10:38:37 AM',\n",
    "                        'nautical_twilight_end': '11:18:00 PM',\n",
    "                        'astronomical_twilight_begin': '10:06:05 AM',\n",
    "                        'astronomical_twilight_end': '11:50:33 PM'},\n",
    "             'status': 'OK',\n",
    "             'lat-lng': (42.3601, -71.0589),\n",
    "             'date': '2022-02-15'}\n",
    "\n",
    "# test without timezone conversion\n",
    "sun_series = clean_sun_dict(sun_dict, timezone_to='GMT')\n",
    "\n",
    "sun_series_exp = pd.Series(\n",
    "{'date': datetime(year=2022, month=2, day=15),\n",
    "'lat-lng': (42.3601, -71.0589),\n",
    "'sunrise': time(hour=11, minute=38, second=48),\n",
    "'sunrise_hr': 11.646666666666667,\n",
    "'sunset': time(hour=22, minute=17, second=50),\n",
    "'sunset_hr': 22.297222222222224})\n",
    "\n",
    "assert sun_series.eq(sun_series_exp).all(), 'clean_sun_dict() error (GMT)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with timezone conversion\n",
    "sun_series = clean_sun_dict(sun_dict, timezone_to='US/Eastern',)\n",
    "\n",
    "sun_series_exp = pd.Series(\n",
    "{'date': datetime(year=2022, month=2, day=15),\n",
    "'lat-lng': (42.3601, -71.0589),\n",
    "'sunrise': time(hour=6, minute=38, second=48),\n",
    "'sunrise_hr': 6.6466666666666665,\n",
    "'sunset': time(hour=17, minute=17, second=50),\n",
    "'sunset_hr': 17.297222222222224})\n",
    "\n",
    "assert sun_series.eq(sun_series_exp).all(), 'clean_sun_dict() error (EST)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4: Getting our Data Frame (5 points)\n",
    "\n",
    "Write the `get_annual_sun_data()` function so that it produces the outputs shown below.  This function should make use of:\n",
    " - `get_sunrise_sunset()`\n",
    " - `clean_sun_dict()`\n",
    "   \n",
    "as built above. I will start the function for you to help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet:\n",
    "\n",
    "```python\n",
    "loc_dict = {'Boston': (42.3601, -71.0589, 'US/Eastern'),\n",
    "            'Lusaka': (-15.3875, 28.3228, 'Africa/Lusaka'),\n",
    "            'Sydney': (-33.8688, 151.2093, 'Australia/Sydney')}\n",
    "df_annual_sun = get_annual_sun_data(loc_dict, year=2021, period_day=30)\n",
    "df_annual_sun.head(6)\n",
    "```\n",
    "\n",
    "should generate:\n",
    "\n",
    "|    |   city |       date |              lat-lng |  sunrise | sunrise_hr |   sunset | sunset_hr |\n",
    "|---:|-------:|-----------:|---------------------:|---------:|-----------:|---------:|----------:|\n",
    "|  0 | Boston | 2021-01-01 |  (42.3601, -71.0589) | 07:11:49 |   7.196944 | 16:24:12 | 16.403333 |\n",
    "|  1 | Lusaka | 2021-01-01 |  (-15.3875, 28.3228) | 05:38:33 |   5.642500 | 18:42:09 | 18.702500 |\n",
    "|  2 | Sydney | 2021-01-01 | (-33.8688, 151.2093) | 05:46:24 |   5.773333 | 20:10:53 | 20.181389 |\n",
    "|  3 | Boston | 2021-01-31 |  (42.3601, -71.0589) | 06:56:43 |   6.945278 | 16:58:42 | 16.978333 |\n",
    "|  4 | Lusaka | 2021-01-31 |  (-15.3875, 28.3228) | 05:55:43 |   5.928611 | 18:44:35 | 18.743056 |\n",
    "|  5 | Sydney | 2021-01-31 | (-33.8688, 151.2093) | 06:14:24 |   6.240000 | 20:02:42 | 20.045000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could be useful\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annual_sun_data(loc_dict, year=2021, period_day=30): \n",
    "    \"\"\" pulls evenly spaced sunrise / sunsets from API over year per city\n",
    "    \n",
    "    Args:\n",
    "        loc_dict (dict): keys are cities, values are tuples of \n",
    "            (lat, lon, tz_str) where tz_str is a timezone\n",
    "            string included in pytz.all_timezones\n",
    "        year (int): year to query\n",
    "        period_day (int): how many days between data queries\n",
    "            (i.e. period_day=1 will get every day for the year)\n",
    "            \n",
    "    Returns:\n",
    "        df_annual_sun (DataFrame): each row represents a \n",
    "            sunrise / sunset datapoint, see get_sunrise_sunset()\n",
    "    \"\"\"\n",
    "\n",
    "    cycle_day = pd.to_datetime(f'{year}-01-01')\n",
    "    cycle_city = loc_dict.keys()\n",
    "    df_annual_sun = pd.DataFrame()\n",
    "    \n",
    "    while cycle_day.year == year:\n",
    "        for city in cycle_city:\n",
    "            city_series = pd.Series({'city': city})\n",
    "\n",
    "            #continue the for loop, using the two functions you've already written\n",
    "\n",
    "        #continue the while loop until you reach the end of the year\n",
    "    \n",
    "    # remove the pass below and include a return statement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dict = {'Boston': (42.3601, -71.0589, 'US/Eastern'),\n",
    "            'Lusaka': (-15.3875, 28.3228, 'Africa/Lusaka'),\n",
    "            'Sydney': (-33.8688, 151.2093, 'Australia/Sydney')}\n",
    "\n",
    "# you may find that setting period_day to a larger value is quicker for debug\n",
    "# period_day=5 takes about a minute or so given the API does 2-3 requests / sec\n",
    "df_annual_sun = get_annual_sun_data(loc_dict, year=2021, period_day=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_sun.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.5: Plotting the data (5 points)\n",
    "\n",
    "Using [plt.fillbetween()](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.fill_between.html), like [this example](https://colab.research.google.com/drive/1eYuuwGwQKEa6x84fqpdVlf46sXLDmhCZ?usp=sharing), write the `plot_daylight()` function so that:\n",
    "\n",
    "``` python\n",
    "plot_daylight(df_annual_sun)\n",
    "```\n",
    "\n",
    "produces a similar graph to:\n",
    "\n",
    "<img src=\"https://i.ibb.co/CBhWtCY/newdaylight.png\" alt=\"newdaylight\" style=\"width: 500px;\"/>\n",
    "\n",
    "Be sure that your graph displays in Jupyter notebook (no need to save it in another form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules you might use\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "\n",
    "def plot_daylight(df_annual_sun):\n",
    "    \"\"\" produces a plot of daylight seen across cities\n",
    "    \n",
    "    Args:\n",
    "        df_annual_sun (DataFrame): each row represents a \n",
    "            sunrise / sunset datapoint, see get_sunrise_sunset()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Function content here, remove pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about a minute to run with period_day=7, worth the wait to characterize\n",
    "# the sudden jumps due to daylight savings times\n",
    "df_annual_sun = get_annual_sun_data(loc_dict, year=2021, period_day=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_daylight(df_annual_sun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Spotify API (Spotipy)\n",
    "\n",
    "**Note**: The following is copied verbatim from the end of Day2_APIs on Canvas.\n",
    "\n",
    "The Spotify API is quite powerful and gives us access to any song/artist in its libraries, plus even more information that you might not have thought of. There is also a module that has been created to access the API within python. Open up a terminal (or do it in jupyter notebook; this is a magic module) and run:\n",
    "\n",
    "`pip install spotipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after installation\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with OpenWeather, we need to make an account [here](https://developer.spotify.com/) (this is essentially the same as making a regular Spotify account) and then get an API key (Spotify requires two things, actually, a Client ID and a secret key). At the above website, go to:\n",
    "\n",
    "- Dashboard\n",
    "- Log into your Spotify account (make one if you don't have one)\n",
    "- Accept the terms of using the API\n",
    "- Create an app (you can call it anything, I called mine `DS3000_Spotify`)\n",
    "- Get a client ID (mine is `592acf2d2dc84d94bbc652f2f1d72375`, though it is usually good practice to **not** share this) and a client secret (**never share this with anyone**: save it in a separate file like we did with our OpenWeather API key earlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists a file `spotify_secret.py` in same directory as this jupyter notebook which contains:\n",
    "    \n",
    "    secret = 'professorgerberssecretspotify'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have named your file spotify_secret.py\n",
    "from spotify_secret import secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "# Make sure you use your OWN client ID (DO NOT leave mine in there!!)\n",
    "cid = '592acf2d2dc84d94bbc652f2f1d72375'\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1: Uniform Resource Identifiers (URI) (5 points)\n",
    "\n",
    "An important component of using the Spotify API is the use of the uniform resource identifiers, pointing at each object in the API. We need a URI to perform any function with the API referring to an object in Spotify. The URI of any Spotify object is contained in its shareable link. For example, the link to the Global top songs playlist, when found from the Spotify desktop application, is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you receive a couldn't read cache or write token error, it should simply be a warning and not be a problem\n",
    "playlist_link = \"https://open.spotify.com/playlist/37i9dQZF1DXcBWIGoYBM5M\"\n",
    "playlist_URI = playlist_link.split(\"/\")[-1].split(\"?\")[0]\n",
    "track_uris = [x[\"track\"][\"uri\"] for x in sp.playlist_tracks(playlist_URI)[\"items\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at a single track from the playlist\n",
    "# it is commented out because it will produce a lot of output! You can uncomment it out to see what it looks like.\n",
    "\n",
    "# sp.playlist_tracks(playlist_URI)[\"items\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Getting to Know You form, I asked what the theme song for the TV show of your life would be. Below is the url for the Spotify playlist created from those songs. Read it into python the way we did the above playlist.\n",
    "\n",
    "`playlist_link = \"https://open.spotify.com/playlist/6Bl2siSxFI0mmdzOwy562o\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: Build a dictionary of track info (5 points)\n",
    "\n",
    "I have initialized the `playlist_dict` dictionary for you below, complete with keys and empty lists. Use a for loop to loop through the tracks in our class's playlist and `.append()` the associated values for each track to each of the lists, building out the dictionary. You may write a function to do this, but you don't have to (I didn't). **Note**: if you know of a faster way to do this than with a for loop, you are welcome to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_dict = {'track_uri': list(),\n",
    "                'track_name': list(),\n",
    "                'artist_uri': list(),\n",
    "                'artist_name': list(),\n",
    "                'artist_pop': list(),\n",
    "                'artist_genres': list(),\n",
    "                'album': list(),\n",
    "                'track_pop': list()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3: Extracting Features from Tracks (5 points)\n",
    "\n",
    "Now that we have a list of track URIs, we can extract features from these tracks. Spotify has a list of these features for each of its tracks, from analysis of the audio. We can access these with a single method of the spotify object `audio_features(uri)`. This gives us a list of mostly numerical features that we can use for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that .audio_features returns a dictionary in a list of 1\n",
    "sp.audio_features(playlist_dict['track_uri'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have initiated an empty `song_dict` for you below. Loop through the track URIs of our `playlist_dict` from the previous part then for each song, loop through the audio features and add corresponding key-value pairs to the dictionary. Finally, after you have created the dictionary, cast it to a Data Frame called `song_df` using pandas. **Note**: if you know of a faster way to do this than with a nested for loop, you are welcome to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "song_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the below to see if you completed your task\n",
    "# song_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.4: Cleaning/manipulating (5 points)\n",
    "\n",
    "Look at the information in `playlist_dict` and the information in `song_df`. What information would you like to add to `song_df` from `playlist_dict`? What information is in `song_df` that is not very useful?\n",
    "\n",
    "**Briefly discuss** your answers to those two questions in a markdown cell, then make the changes to `song_df`. That is: add the columns to the Data Frame that you wish to add, and delete the columns that are not useful. Keep in mind that there are many \"correct\" answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.5: Plotting (5 points)\n",
    "\n",
    "Using plotly, make two plots (in separate code cells):\n",
    "- of energy (x-axis) versus danceability (y-axis) that has the song title as hover data.\n",
    "- of acousticness (x-axis) versus loudness (y-axis) that has the song title as hover data.\n",
    "\n",
    "**Then**, in a markdown cell, discuss the relationships (if any) you see in these plots and whether those relationships make sense to you/what you would have expected to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Web Scraping Fantasy Football\n",
    "\n",
    "## Part 3.1: Get a table (5 points)\n",
    "\n",
    "In Fantasy Football, real NFL players gain points for fans at home based on some scoring criteria (which differs based on the website, but in all cases more points is better). Use the `pd.read_html` function to pull in the table located at the following url from Week 1 of current Yahoo Fantasy Football season: https://football.fantasysports.yahoo.com/f1/whoshot?pos=ALL&week=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2: Build a clean Data Frame (10 points)\n",
    "\n",
    "Write a function `clean_nfl_df()` which takes the table from Part 3.1 as input and :\n",
    "\n",
    "- breaks up the `Name` column into three columns: `Name`, `Team`, `Pos` (position). For example, the first couple observations under the `Name` column as it is read in by default should be:\n",
    "\n",
    "| Name                                                      |\n",
    "|----------------------------------------------------------:|\n",
    "| New Player Note  Tyreek Hill Mia - WR Final W 36-34 @ LAC |\n",
    "| No new player Notes Dallas Dal - DEF Final W 40-0 @ NYG   |\n",
    "\n",
    "    this should be replaced with three columns:\n",
    "\n",
    "| Name             | Team | Pos |\n",
    "|-----------------:|-----:|----:|\n",
    "| Tyreek Hill      | Mia  | WR  |\n",
    "| Dallas           | Dal  | DEF |\n",
    "\n",
    "- breaks up the `Stats` column into different numeric columns based on the statistics. There should be:\n",
    "    * `Pass Yds`\n",
    "    * `Rec Yds`\n",
    "    * `Rush Yds`\n",
    "    * `Pass TD`\n",
    "    * `Rec TD`\n",
    "    * `Rush TD`\n",
    "    * `Rec`\n",
    "    * `Sack`\n",
    "    * `Int`\n",
    "    * `Fum Rec`\n",
    "    \n",
    "    so that the first couple observations:\n",
    "    \n",
    "| Stats                               |\n",
    "|------------------------------------:|\n",
    "| 11 Rec, 215 Rec Yds, 2 Rec TD       |\n",
    "| 7 Sack, 2 Int, 1 Fum Rec            |\n",
    "\n",
    "    becomes\n",
    "    \n",
    "| Pass Yds | Rec Yds | Rush Yds | Pass TD | Rec TD | Rush TD | Rec | Sack | Int | Fum Rec |\n",
    "|---------:|--------:|---------:|--------:|-------:|--------:|----:|-----:|----:|--------:|\n",
    "|   NaN    |  215    |    NaN   |   NaN   | 2      | NaN     | 11  | NaN  | NaN | NaN     |\n",
    "|   NaN    |  NaN    |    NaN   |   NaN   | NaN    | NaN     | NaN | 7    | 2   | 1       |\n",
    "\n",
    "- produces the final clean data frame whose first two rows look like:\n",
    "\n",
    "| Name             | Team | Pos | Pass Yds | Rec Yds | Rush Yds | Pass TD | Rec TD | Rush TD | Rec | Sack | Int | Fum Rec | Fan Pts   |\n",
    "|-----------------:|-----:|----:|---------:|--------:|---------:|--------:|-------:|--------:|----:|-----:|----:|--------:|----------:|\n",
    "| Tyreek Hill      | Mia  | WR  |   NaN    |  215    |    NaN   |   NaN   | 2      | NaN     | 11  | NaN  | NaN | NaN     | 39.00 |\n",
    "| Dallas           | Dal  | DEF |   NaN    |  NaN    |    NaN   |   NaN   | NaN    | NaN     | NaN | 7    | 2   | 1       | 37.00 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3: More Cleaning (5 points)\n",
    "\n",
    "Go to the url from Part 2.1 in the browser and examine how the url changes when different positions or weeks are selected. Then, write the function `weekly_nfl_df()` which takes two arguments `pos` and `week`, and which uses your `clean_nfl_df()` function from Part 2.1 to produce a clean data frame for any position and any week (depending on when you complete this, there have only been 2-5 weeks so far; thus when you test this function only use 1 or 2 to be safe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.4: Using the function (5 points)\n",
    "\n",
    "Use your `weekly_nfl_df()` function from Part 2.3 to create a single data frame that includes the top fantasy Quarterbacks (`pos = 'QB'`) for the first 3 weeks of this season. **Note**: there may be multiple ways to do this, and you **must** wait until the morning of September 26 to be able to do all three weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.5: Plotting and Interpretation (5 points)\n",
    "\n",
    "Make a graph which plots `Pass Yds` on the x-axis against `Fan Pts` on the y-axis, and colors the points based on `Pass TD`. Use `plotly` and include the `Name` and `Team` as `hover_data`. Make sure the graph is well labeled, titled, and includes a legend. Then, in a Markdown cell, discuss in **at least 3** sentences your interpretation of the graph.\n",
    "\n",
    "- **Note**: if you are not an american football fan, in brief the Quarterback's role is to throw the ball to other players in the hopes of scoring a touchdown (if you want a much more technical description, you may also read a bit [here](https://en.wikipedia.org/wiki/Quarterback))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.6: More Plotting and Interpretation(5 points)\n",
    "\n",
    "Use the `weekly_nfl_df()` function to create data frames for each of the four main offensive positions (QB, WR, RB, TE) **for the first four weeks** and then create, using subplots in a single plot, histograms for each positions' `Fan Pts`. Make sure the subplots are on the same scale, well labeled, and titled. Discuss the results in a few short sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Sketch and Begin Implementing a Pipeline\n",
    "\n",
    "We wish to create a data frame that includes all the spells for each class (a \"class\" is something like a \"wizard\", or a \"bard\") in Dungeons and Dragons 5th Edition, which you can find [here](http://dnd5e.wikidot.com/). Your final data frame should look something like:\n",
    "\n",
    "| Class     | Level     | Spell Name    | School      | Casting Time | Range                | Duration      | Components |\n",
    "|----------:|----------:|--------------:|------------:|-------------:|---------------------:|--------------:|-----------:|\n",
    "| Artificer | Cantrip   | Acid Splash   | Conjuration | 1 Action     | 60 Feet              | Instantaneous | V, S       |\n",
    "| Artificer | Cantrip   | Booming Blade | Evocation   | 1 Action     | Self (5-foot radius) | 1 Round       | S, M       |\n",
    "| ...       | ...       | ...           | ...         | ...          | ...                  | ...           | ...        |\n",
    "| Wizard    | 9th Level | Wish          | Conjuration | 1 Action     | Self                 | Instantaneous | V          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.1: Poking Around (5 points)\n",
    "\n",
    "Go to the D&D 5th Edition linked above. Scroll down to the \"All Spells\" list and click on \"Artificer Spells\", spend a moment looking around at the page, then \"Bard Spells\" to do the same, and make note of the url of each. What do you note about the pages and url that should be pretty convenient for scraping the data we are interested in for all different types of spells? Discuss anything else you might notice about the pages that may be either tricky or convenient to deal with. Note that in our desired data frame, we include the \"Class\" and \"Level\" for each spell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.2: Sketch the Pipeline (5 points)\n",
    "\n",
    "First, in a markdown cell, write a bullet point list of tasks we need to get the data frame we want. I'll give you what the first bullet point should be, and you fill in the rest (there may be only one more, depending on how efficient you are in describing the tasks...):\n",
    "\n",
    "- Write a function that takes a class (string) as an argument and returns the tables from the class's DND wiki spell page in a dictionary for each spell level\n",
    "- ... \n",
    "\n",
    "Then, in a code cell, define **empty** functions that correspond to the tasks you identified as needing done. For example, the function for the first bullet point above might start with:\n",
    "\n",
    "```python\n",
    "def get_class_spell_dict(dnd_class):\n",
    "    \"\"\" takes a D&D class (string) and gets the spell tables and saves them in a dictionary\n",
    "    \n",
    "    Args:\n",
    "        dnd_class (str): the D&D class\n",
    "        \n",
    "    Returns:\n",
    "        table_dict (dict): a dictionary of tables, one for each spell level\n",
    "    \"\"\"\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.3: Write the first function (5 points)\n",
    "\n",
    "Go ahead and write the first function that I defined the framework for in part 4.2, and then test it by getting the dictionary of Bard spell tables. Show that it works by printing out the head of the 2nd level Bard spells. Your final calls should be something like:\n",
    "\n",
    "```python\n",
    "bard_spell_tables = get_class_spell_dict(\"bard\")\n",
    "bard_spell_tables['Level 2'].head()\n",
    "```\n",
    "\n",
    "**Note**: depending on how you create the dictionary in your `get_class_spell_dict` function, you may not have `'Level 2'` as the key; that's fine. The top of the table should look like:\n",
    "\n",
    "| Spell Name       | School        | Casting Time | Range                | Duration      | Components |\n",
    "|-----------------:|--------------:|-------------:|---------------------:|--------------:|-----------:|\n",
    "| Aid              | Abjuration    | 1 Action     | 30 Feet              | 8 hours       | V, S, M    |\n",
    "| Animal Messenger | Enchantment   | 1 Action R   | 30 Feet              | 24 hours      | V, S, M    | M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
