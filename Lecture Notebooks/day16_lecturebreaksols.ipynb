{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acb3892",
   "metadata": {},
   "source": [
    "# Lecture Break/Practice 1 Solutions (Random Forest Speed Dater)\n",
    "## It will not run in this notebook; needs to be copied over to the `day16.ipynb` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ac005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# The classifier via cross validation\n",
    "# And the confusion matrix\n",
    "n_splits = 10\n",
    "max_depth = 4\n",
    "\n",
    "# initialize k fold\n",
    "skfold = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "# initialize random forest\n",
    "rf_clf = RandomForestClassifier(max_depth=max_depth, n_estimators=1000)\n",
    "# be careful with n_estimators here; because this is a decent sized data set (6K rows) any higher number of trees will run slowly\n",
    "\n",
    "# initialize y_pred, stores predictions of y\n",
    "y_pred = np.empty_like(y)\n",
    "\n",
    "for train_idx, test_idx in skfold.split(x, y):\n",
    "    # get training data\n",
    "    x_train = x[train_idx, :]\n",
    "    y_train = y[train_idx]\n",
    "    \n",
    "    # get test data    \n",
    "    x_test = x[test_idx, :]\n",
    "    \n",
    "    # fit data\n",
    "    rf_clf = rf_clf.fit(x_train, y_train)\n",
    "    \n",
    "    # estimate on test data\n",
    "    y_pred[test_idx] = rf_clf.predict(x_test)\n",
    "\n",
    "y_feat_list = np.array(['No Match', 'Yes Match'])\n",
    "\n",
    "# build and plot confusion matrix\n",
    "sns.set(font_scale=1.2)\n",
    "conf_mat = confusion_matrix(y_true=y, y_pred=y_pred)\n",
    "conf_mat_disp = ConfusionMatrixDisplay(conf_mat, display_labels=y_feat_list)\n",
    "conf_mat_disp.plot()\n",
    "plt.gcf().set_size_inches(7, 7)\n",
    "plt.grid(False)\n",
    "plt.suptitle('max_depth=4 decision trees in a random forest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a06ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_acc_sens_spec(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c594a",
   "metadata": {},
   "source": [
    "# Is this useful?\n",
    "\n",
    "It looks like it's pretty useful! Not only do we have (see below) very high accuracy (overall correct prediction proportion) and sensitivity (true positive rate: the proportion of correctly predicted matches) we also have a perfect sensitivity (we have no false positives; everyone we predict wouldn't go on a date, didn't). Keep in mind, however, that since this is a **random** forest, we may not get the exact same results every time we run it, but as long as the results are consistent if we run it a few times, they should be trustworthy. However, be careful if the feature importance varies greatly (see below), as that might indicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feat_import(x_feat_list, rf_clf.feature_importances_, limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6145a5",
   "metadata": {},
   "source": [
    "It seems that (generally) how important the speed dater finds certain activities are more important than the partner's ratings of the importance of different qualities, or of the partner's ratings of the speed dater themselves. Gender and whether the two are the same race also seem to be relatively important (they are usually in the top 10, and we have 33 total x features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4fd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4907f78",
   "metadata": {},
   "source": [
    "# Lecture Break/Practice 2 Solutions (Regression with EV data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df_ev['Acceleration']).reshape(-1, 1)\n",
    "y = np.array(df_ev['Top Speed'])\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(x, y) \n",
    "\n",
    "# get the slope\n",
    "slope = reg.coef_[0]\n",
    "\n",
    "# get the intercept\n",
    "intercept = reg.intercept_\n",
    "\n",
    "y_pred = reg.predict(x) # same as y_pred = slope * x + intercept\n",
    "\n",
    "show_fit(x, y, slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2fd8a",
   "metadata": {},
   "source": [
    "The slope can be interpreted as for every additional second it takes an EV to reach 100 km/hr, the Top Speed of the EV decreases by 10.49 km/hr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=y, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe6a0b",
   "metadata": {},
   "source": [
    "76.1% of the variability in Top Speed can be explained by Acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check independence\n",
    "errors = y - y_pred\n",
    "plt.scatter(x = range(len(y)), y = errors)\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('errors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c6f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check constant variance\n",
    "plt.scatter(x = x, y = errors)\n",
    "plt.xlabel('Acceleration')\n",
    "plt.ylabel('errors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check normality\n",
    "stats.probplot(errors, dist=\"norm\", plot=py)\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea1659",
   "metadata": {},
   "source": [
    "Independence and normality look fairly well met, but constant variance does not. In fact, there is some curving to the errors, which suggests there is another feature that impacts Top Speed which we are not accounting for. This should lead us to examine moving to a multiple and/or polynomial regression framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad340b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a3c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
